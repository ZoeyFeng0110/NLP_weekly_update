{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and set up logging\n",
    "import gensim \n",
    "import logging\n",
    "import glob, os\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory containing all source texts for training the model \n",
    "data_dir=\"/Users/oliviafeng/Desktop/uchi/digital_text2/rick_and_morty_episodes_txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_4.txt = 25112 chars\n",
      "episode_5.txt = 23781 chars\n",
      "episode_7.txt = 26417 chars\n",
      "episode_6.txt = 21026 chars\n",
      "episode_2.txt = 32767 chars\n",
      "episode_3.txt = 21536 chars\n",
      "episode_1.txt = 26824 chars\n",
      "episode_0.txt = 30370 chars\n",
      "episode_10.txt = 32767 chars\n",
      "episode_11.txt = 29633 chars\n",
      "episode_13.txt = 644 chars\n",
      "episode_12.txt = 30026 chars\n",
      "episode_16.txt = 21591 chars\n",
      "episode_17.txt = 20546 chars\n",
      "episode_15.txt = 30903 chars\n",
      "episode_14.txt = 25946 chars\n",
      "episode_28.txt = 17601 chars\n",
      "episode_25.txt = 19829 chars\n",
      "episode_19.txt = 25438 chars\n",
      "episode_18.txt = 29884 chars\n",
      "episode_24.txt = 32767 chars\n",
      "episode_26.txt = 32767 chars\n",
      "episode_27.txt = 32767 chars\n",
      "episode_23.txt = 31211 chars\n",
      "episode_22.txt = 32767 chars\n",
      "episode_20.txt = 12670 chars\n",
      "episode_21.txt = 26546 chars\n",
      "episode_8.txt = 27402 chars\n",
      "episode_9.txt = 1480 chars\n"
     ]
    }
   ],
   "source": [
    "#data_dir=\"corpus\"\n",
    "os.chdir(data_dir)\n",
    "documents = list()\n",
    "for filename in glob.glob(\"*.txt\"):\n",
    "    filedata = open(filename, 'r').read()\n",
    "    print(filename + \" = \" + str(len(filedata)) + \" chars\")\n",
    "    documents = documents + filedata.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(The sky is purple and tumultuous as the sounds of Rick and Morty running and panting are heard\n",
      "----\n",
      "(The sky is purple and tumultuous as the sounds of Rick and Morty running and panting are heard\n"
     ]
    }
   ],
   "source": [
    "# Check to see that the first sentence is correct\n",
    "print(documents[0])\n",
    "\n",
    "docs2 = []\n",
    "# remove all the \"\\n\"s\n",
    "for doc in documents:\n",
    "    newdoc = doc.replace(\"\\n\", \" \")\n",
    "    docs2.append(newdoc)\n",
    "\n",
    "# one can also do text preprocessing here: lowercase, lemmatize, remove stopwords & punctuation etc\n",
    "\n",
    "print(\"----\")\n",
    "print(docs2[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open(\"/Users/oliviafeng/Desktop/uchi/digital_text2/word2vec_own_work/stop_words.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    custom_stopwords = set(line.strip().lower() for line in f if line.strip())\n",
    "\n",
    "cleaned_sentences = []\n",
    "\n",
    "for doc in docs2:\n",
    "    spacy_doc = nlp(doc)\n",
    "    tokens =[]\n",
    "    for token in spacy_doc:\n",
    "        lemma = token.lemma_.lower()\n",
    "        if (\n",
    "            lemma in custom_stopwords\n",
    "            or token.is_punct\n",
    "            or not lemma.isalpha()\n",
    "            or len(lemma) <= 1  \n",
    "        ):\n",
    "            continue\n",
    "\n",
    "   \n",
    "        lemma = re.sub(r\":$\", \"\", lemma)\n",
    "\n",
    "        tokens.append(lemma)\n",
    "\n",
    "    cleaned_sentences.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 23:40:14,147 : INFO : collecting all words and their counts\n",
      "2025-04-14 23:40:14,148 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2025-04-14 23:40:14,212 : INFO : PROGRESS: at sentence #10000, processed 61264 words and 44294 word types\n",
      "2025-04-14 23:40:14,222 : INFO : collected 49740 token types (unigram + bigrams) from a corpus of 69853 words and 11231 sentences\n",
      "2025-04-14 23:40:14,222 : INFO : merged Phrases<49740 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2025-04-14 23:40:14,223 : INFO : Phrases lifecycle event {'msg': 'built Phrases<49740 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000> in 0.08s', 'datetime': '2025-04-14T23:40:14.223309', 'gensim': '4.3.1', 'python': '3.10.17 | packaged by conda-forge | (main, Apr 10 2025, 22:15:49) [Clang 18.1.8 ]', 'platform': 'macOS-14.6.1-x86_64-i386-64bit', 'event': 'created'}\n",
      "2025-04-14 23:40:14,223 : INFO : exporting phrases from Phrases<49740 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2025-04-14 23:40:14,302 : INFO : FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<183 phrases, min_count=5, threshold=10.0> from Phrases<49740 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000> in 0.08s', 'datetime': '2025-04-14T23:40:14.302430', 'gensim': '4.3.1', 'python': '3.10.17 | packaged by conda-forge | (main, Apr 10 2025, 22:15:49) [Clang 18.1.8 ]', 'platform': 'macOS-14.6.1-x86_64-i386-64bit', 'event': 'created'}\n",
      "2025-04-14 23:40:14,304 : INFO : collecting all words and their counts\n",
      "2025-04-14 23:40:14,305 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2025-04-14 23:40:14,422 : INFO : PROGRESS: at sentence #10000, processed 59024 words and 44809 word types\n",
      "2025-04-14 23:40:14,441 : INFO : collected 50331 token types (unigram + bigrams) from a corpus of 67291 words and 11231 sentences\n",
      "2025-04-14 23:40:14,441 : INFO : merged Phrases<50331 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2025-04-14 23:40:14,442 : INFO : Phrases lifecycle event {'msg': 'built Phrases<50331 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000> in 0.14s', 'datetime': '2025-04-14T23:40:14.442449', 'gensim': '4.3.1', 'python': '3.10.17 | packaged by conda-forge | (main, Apr 10 2025, 22:15:49) [Clang 18.1.8 ]', 'platform': 'macOS-14.6.1-x86_64-i386-64bit', 'event': 'created'}\n",
      "2025-04-14 23:40:14,443 : INFO : exporting phrases from Phrases<50331 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2025-04-14 23:40:14,525 : INFO : FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<166 phrases, min_count=5, threshold=10.0> from Phrases<50331 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000> in 0.08s', 'datetime': '2025-04-14T23:40:14.525207', 'gensim': '4.3.1', 'python': '3.10.17 | packaged by conda-forge | (main, Apr 10 2025, 22:15:49) [Clang 18.1.8 ]', 'platform': 'macOS-14.6.1-x86_64-i386-64bit', 'event': 'created'}\n",
      "2025-04-14 23:40:14,713 : INFO : collecting all words and their counts\n",
      "2025-04-14 23:40:14,714 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2025-04-14 23:40:14,728 : INFO : PROGRESS: at sentence #10000, processed 58775 words, keeping 6747 word types\n",
      "2025-04-14 23:40:14,730 : INFO : collected 7293 word types from a corpus of 67015 raw words and 11231 sentences\n",
      "2025-04-14 23:40:14,731 : INFO : Creating a fresh vocabulary\n",
      "2025-04-14 23:40:14,753 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 7293 unique words (100.00% of original 7293, drops 0)', 'datetime': '2025-04-14T23:40:14.753481', 'gensim': '4.3.1', 'python': '3.10.17 | packaged by conda-forge | (main, Apr 10 2025, 22:15:49) [Clang 18.1.8 ]', 'platform': 'macOS-14.6.1-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2025-04-14 23:40:14,754 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 67015 word corpus (100.00% of original 67015, drops 0)', 'datetime': '2025-04-14T23:40:14.754583', 'gensim': '4.3.1', 'python': '3.10.17 | packaged by conda-forge | (main, Apr 10 2025, 22:15:49) [Clang 18.1.8 ]', 'platform': 'macOS-14.6.1-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2025-04-14 23:40:14,789 : INFO : deleting the raw counts dictionary of 7293 items\n",
      "2025-04-14 23:40:14,790 : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2025-04-14 23:40:14,790 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 57565.846870119974 word corpus (85.9%% of prior 67015)', 'datetime': '2025-04-14T23:40:14.790695', 'gensim': '4.3.1', 'python': '3.10.17 | packaged by conda-forge | (main, Apr 10 2025, 22:15:49) [Clang 18.1.8 ]', 'platform': 'macOS-14.6.1-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2025-04-14 23:40:14,848 : INFO : estimated required memory for 7293 words and 300 dimensions: 21149700 bytes\n",
      "2025-04-14 23:40:14,849 : INFO : resetting layer weights\n",
      "2025-04-14 23:40:14,863 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-04-14T23:40:14.863897', 'gensim': '4.3.1', 'python': '3.10.17 | packaged by conda-forge | (main, Apr 10 2025, 22:15:49) [Clang 18.1.8 ]', 'platform': 'macOS-14.6.1-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2025-04-14 23:40:14,864 : INFO : Word2Vec lifecycle event {'msg': 'training model with 20 workers on 7293 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=8 shrink_windows=True', 'datetime': '2025-04-14T23:40:14.864568', 'gensim': '4.3.1', 'python': '3.10.17 | packaged by conda-forge | (main, Apr 10 2025, 22:15:49) [Clang 18.1.8 ]', 'platform': 'macOS-14.6.1-x86_64-i386-64bit', 'event': 'train'}\n",
      "2025-04-14 23:40:15,037 : INFO : EPOCH 0: training on 67015 raw words (57549 effective words) took 0.2s, 360282 effective words/s\n",
      "2025-04-14 23:40:15,200 : INFO : EPOCH 1: training on 67015 raw words (57533 effective words) took 0.2s, 381938 effective words/s\n",
      "2025-04-14 23:40:15,426 : INFO : EPOCH 2: training on 67015 raw words (57494 effective words) took 0.2s, 271795 effective words/s\n",
      "2025-04-14 23:40:15,846 : INFO : EPOCH 3: training on 67015 raw words (57574 effective words) took 0.4s, 142156 effective words/s\n",
      "2025-04-14 23:40:16,308 : INFO : EPOCH 4: training on 67015 raw words (57677 effective words) took 0.4s, 129224 effective words/s\n",
      "2025-04-14 23:40:16,869 : INFO : EPOCH 5: training on 67015 raw words (57575 effective words) took 0.5s, 105781 effective words/s\n",
      "2025-04-14 23:40:17,416 : INFO : EPOCH 6: training on 67015 raw words (57579 effective words) took 0.5s, 110086 effective words/s\n",
      "2025-04-14 23:40:17,875 : INFO : EPOCH 7: training on 67015 raw words (57638 effective words) took 0.4s, 134318 effective words/s\n",
      "2025-04-14 23:40:18,292 : INFO : EPOCH 8: training on 67015 raw words (57609 effective words) took 0.4s, 144247 effective words/s\n",
      "2025-04-14 23:40:18,742 : INFO : EPOCH 9: training on 67015 raw words (57512 effective words) took 0.4s, 134629 effective words/s\n",
      "2025-04-14 23:40:18,744 : INFO : Word2Vec lifecycle event {'msg': 'training on 670150 raw words (575740 effective words) took 3.9s, 148412 effective words/s', 'datetime': '2025-04-14T23:40:18.744299', 'gensim': '4.3.1', 'python': '3.10.17 | packaged by conda-forge | (main, Apr 10 2025, 22:15:49) [Clang 18.1.8 ]', 'platform': 'macOS-14.6.1-x86_64-i386-64bit', 'event': 'train'}\n",
      "2025-04-14 23:40:18,745 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=7293, vector_size=300, alpha=0.025>', 'datetime': '2025-04-14T23:40:18.745155', 'gensim': '4.3.1', 'python': '3.10.17 | packaged by conda-forge | (main, Apr 10 2025, 22:15:49) [Clang 18.1.8 ]', 'platform': 'macOS-14.6.1-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rick', 'morty', 'jerry', 'get', 'summer', 'know', 'beth', 'oh', 'like', 'right']\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec, Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "#documents = [\"the mayor of new york was there\", \"human computer interaction and machine learning has now become a trending research area\",\"human computer interaction is interesting\",\"human computer interaction is a pretty interesting subject\", \"human computer interaction is a great and new subject\", \"machine learning can be useful sometimes\",\"new york mayor was present\", \"I love machine learning because it is a new subject area\", \"human computer interaction helps people to get user friendly applications\"]\n",
    "\n",
    "# sentence_stream = [doc.split(\" \") for doc in docs2]  #documents\n",
    "\n",
    "trigram_sentences_project = []\n",
    "\n",
    "bigram = Phraser(Phrases(cleaned_sentences))\n",
    "trigram = Phraser(Phrases(bigram[cleaned_sentences]))\n",
    "\n",
    "for sent in cleaned_sentences:\n",
    "    bigrams_ = bigram[sent]\n",
    "    trigrams_ = trigram[bigram[sent]]\n",
    "    trigram_sentences_project.append(trigrams_)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 1    # Minimum word count                        \n",
    "num_workers = 20      # Number of threads to run in parallel\n",
    "context = 8           # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "skip_grams = 1        # 0 for CBOW, 1 for skip-gramsword2vec.Word2Vec\n",
    "\n",
    "model = word2vec.Word2Vec(trigram_sentences_project, workers=num_workers, \\\n",
    "            vector_size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling, sg = skip_grams, epochs=10)\n",
    "\n",
    "# , epochs=10\n",
    "\n",
    "vocab = list(model.wv.index_to_key) #vocab.keys()\n",
    "print(vocab[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7293\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of items in our model's vocabulary\n",
    "print(len(model.wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bitch', 0.9737847447395325),\n",
       " ('ah', 0.9625250101089478),\n",
       " ('motherfucker', 0.961152195930481),\n",
       " ('shut', 0.9549633264541626),\n",
       " ('whoa_whoa', 0.9543323516845703),\n",
       " ('monster', 0.9541509747505188),\n",
       " ('aah', 0.9520564079284668),\n",
       " ('whoa', 0.9516603350639343),\n",
       " ('feel_good', 0.951093316078186),\n",
       " ('fucking', 0.9504562616348267)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"portal\"\n",
    "w2 = \"morty\"\n",
    "w3 = \"rick\"\n",
    "w4 = \"pickle\"\n",
    "w5 = \"fuck\"\n",
    "# model.wv.most_similar (positive=w1)\n",
    "# model.wv.most_similar (positive=w2)\n",
    "# model.wv.most_similar (positive=w3)\n",
    "# model.wv.most_similar (positive=w4)\n",
    "model.wv.most_similar (positive=w5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ethan: 0.8530\n"
     ]
    }
   ],
   "source": [
    "# Check the \"most similar words\", using the default \"cosine similarity\" measure.\n",
    "\n",
    "result = model.wv.most_similar(positive=[\"morty\", \"summer\"], negative=['rick'])\n",
    "\n",
    "most_similar_key, similarity = result[0]  # look at the first match\n",
    "\n",
    "print(f\"{most_similar_key}: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cosine_similarity is:[('ethan', 0.8529946804046631), ('cry', 0.8318160176277161), ('roars', 0.8191099166870117), ('aah', 0.8184424638748169), ('gasp', 0.8167917728424072), ('god', 0.8167688250541687), ('sorry', 0.8124842643737793), ('bye', 0.8113460540771484), ('kiss', 0.8111793994903564), ('ugh', 0.8102681636810303)]\n",
      "the cosmul_similarity is:[('ethan', 0.9562269449234009), ('cry', 0.9454898238182068), ('roars', 0.938749372959137), ('aah', 0.9385322332382202), ('gasp', 0.937446117401123), ('god', 0.9368084669113159), ('sorry', 0.9352295994758606), ('bye', 0.9345331192016602), ('kiss', 0.9342777729034424), ('ugh', 0.9341124892234802)]\n"
     ]
    }
   ],
   "source": [
    "positive = [\"morty\", \"summer\"]\n",
    "negative = [\"rick\"]\n",
    "\n",
    "# cosine_similarity method\n",
    "cosine_similarity = model.wv.most_similar(positive=positive, negative=negative)\n",
    "print(f\"the cosine_similarity is:{cosine_similarity}\")\n",
    "\n",
    "# cosmul method\n",
    "cosmul_similarity = model.wv.most_similar_cosmul(positive=positive, negative=negative)\n",
    "print(f\"the cosmul_similarity is:{cosmul_similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('love', 0.0012793476),\n",
       " ('morty_jr', 0.0011305797),\n",
       " ('life', 0.0010671734),\n",
       " ('would', 0.0009596353),\n",
       " ('tell', 0.00090086955),\n",
       " ('even', 0.0008906037),\n",
       " ('planet', 0.00082504586),\n",
       " ('father', 0.00081498094),\n",
       " ('mean', 0.00075664005),\n",
       " ('kind', 0.0007543548),\n",
       " ('could', 0.00075212045),\n",
       " ('bad', 0.00070563925),\n",
       " ('much', 0.0006552428),\n",
       " ('need', 0.0006460995),\n",
       " ('maybe', 0.00064282544),\n",
       " ('whole', 0.00063223013),\n",
       " ('son', 0.0006279958),\n",
       " ('earth', 0.0006078488),\n",
       " ('dada', 0.00060304324),\n",
       " ('something', 0.00059853896),\n",
       " ('world', 0.0005837073),\n",
       " ('gazorpian', 0.00058287894),\n",
       " ('die', 0.0005733098),\n",
       " ('ever', 0.0005733026),\n",
       " ('real', 0.0005696367),\n",
       " ('family', 0.0005617296),\n",
       " ('want', 0.00054931734),\n",
       " ('horse', 0.0005467827),\n",
       " ('matter', 0.00053397886),\n",
       " ('conquer', 0.00053298543)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_output_word([\"I\",\"love\",\"life\"], topn=30)  # also the most basic way one could implement text completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change these locations to somewhere on your machine\n",
    "tensorsfp = \"/Users/oliviafeng/Desktop/uchi/digital_text2/word2vec_own_work/tensorsfp.txt\"\n",
    "metadatafp = \"/Users/oliviafeng/Desktop/uchi/digital_text2/word2vec_own_work/metadatafp.txt\"\n",
    "\n",
    "with open( tensorsfp, 'w+') as tensors:\n",
    "    with open( metadatafp, 'w+') as metadata:\n",
    "         for word in model.wv.index_to_key:   #index2word\n",
    "                metadata.write(word + '\\n')\n",
    "                vector_row = '\\t'.join(map(str, model.wv[word]))  #model[word]\n",
    "                tensors.write(vector_row + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
